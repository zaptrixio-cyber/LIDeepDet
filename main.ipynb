{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86a7c02",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6da728",
   "metadata": {},
   "source": [
    "#### Preprocessing & Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f21e19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "from multiprocessing import Pool, cpu_count, current_process\n",
    "\n",
    "# --- Input Paths ---\n",
    "# IMPORTANT: Update this path to your Celeb-DF dataset location\n",
    "BASE_DIR = \"/mnt/data/CelebDF-V2\"\n",
    "REAL_VIDEOS_PATH = os.path.join(BASE_DIR, \"Celeb-real\")\n",
    "FAKE_VIDEOS_PATH = os.path.join(BASE_DIR, \"Celeb-synthesis\")\n",
    "\n",
    "# --- Output Path ---\n",
    "# This directory will be created to store the processed features\n",
    "PREPROCESSED_FEATURES_PATH = \"/mnt/LIDeepDet Features\"\n",
    "\n",
    "# --- Preprocessing Settings ---\n",
    "MAX_VIDEOS_PER_FOLDER = 890\n",
    "MAX_FRAMES_PER_VIDEO = 200\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# Global MTCNN detector for each process to avoid re-initialization per video\n",
    "# This will be initialized once per worker process\n",
    "global_face_detector = None\n",
    "\n",
    "def init_worker():\n",
    "    \"\"\"Initializes the MTCNN detector for each worker process.\"\"\"\n",
    "    global global_face_detector\n",
    "    if global_face_detector is None:\n",
    "        print(f\"[{current_process().name}] Initializing MTCNN detector...\")\n",
    "        global_face_detector = MTCNN()\n",
    "        print(f\"[{current_process().name}] MTCNN detector initialized.\")\n",
    "\n",
    "def extract_illumination_map_paper(face_crop):\n",
    "    \"\"\"\n",
    "    CORRECTED IMPLEMENTATION.\n",
    "    Creates an illumination map that aligns with the paper's goal: \"preserve the\n",
    "    image's overall structure and smooth texture details\". This is achieved robustly\n",
    "    using a Guided Filter, an edge-preserving smoothing technique.\n",
    "    \"\"\"\n",
    "    if face_crop is None: return None\n",
    "    \n",
    "    # 1. Get initial illumination map M_hat (from Equation 2)\n",
    "    m_hat = np.max(face_crop, axis=-1).astype(np.float32) / 255.0\n",
    "\n",
    "    # 2. Use the original image as a guide to preserve structure\n",
    "    # The guided filter will smooth m_hat, but not across the edges present in the guide\n",
    "    guide_image = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 3. Create and apply the guided filter\n",
    "    # Radius and epsilon are key parameters. A larger radius means more smoothing.\n",
    "    radius = 32\n",
    "    epsilon = 0.01\n",
    "    guided_filter = cv2.ximgproc.createGuidedFilter(guide=guide_image, radius=radius, eps=epsilon)\n",
    "    M = guided_filter.filter(src=m_hat)\n",
    "    \n",
    "    # Normalize for saving and visualization\n",
    "    smoothed_map = cv2.normalize(M, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    smoothed_map = np.uint8(smoothed_map)\n",
    "    return cv2.cvtColor(smoothed_map, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "def extract_face_material_map_paper(face_crop, mask_size=5):\n",
    "    \"\"\"\n",
    "    Creates a face material map using the Pattern of Local Gravitational Force\n",
    "    (PLGF) descriptor as defined in Equations (5-7) of the paper.\n",
    "    The paper's text states the magnitude is used for texture, so this is correct.\n",
    "    \"\"\"\n",
    "    if face_crop is None: return None\n",
    "    gray_face = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY).astype(np.float64)\n",
    "    radius = mask_size // 2\n",
    "    y, x = np.mgrid[-radius:radius+1, -radius:radius+1]\n",
    "    epsilon = 1e-12\n",
    "    denominator = x**2 + y**2 + epsilon\n",
    "    kernel_tx = (np.cos(np.arctan2(y, x))) / denominator\n",
    "    kernel_ty = (np.sin(np.arctan2(y, x))) / denominator\n",
    "    kernel_tx[radius, radius] = 0\n",
    "    kernel_ty[radius, radius] = 0\n",
    "    gx = convolve2d(gray_face, kernel_tx, mode='same', boundary='symm')\n",
    "    gy = convolve2d(gray_face, kernel_ty, mode='same', boundary='symm')\n",
    "    magnitude = np.sqrt(gx**2 + gy**2)\n",
    "    material_map = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    material_map = np.uint8(material_map)\n",
    "    return cv2.cvtColor(material_map, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "def estimate_light_direction(face_crop):\n",
    "    \"\"\"\n",
    "    Estimates the 2D light direction vector based on the Lambertian model\n",
    "    (Paper Section 3.4), assuming the brightest region of the face points\n",
    "    towards the light source. This remains a robust proxy.\n",
    "    \"\"\"\n",
    "    if face_crop is None: return np.array([0.0, 0.0])\n",
    "    gray_face = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "    _, _, _, max_loc = cv2.minMaxLoc(gray_face)\n",
    "    center_x, center_y = gray_face.shape[1] // 2, gray_face.shape[0] // 2\n",
    "    vec = np.array([max_loc[0] - center_x, max_loc[1] - center_y], dtype=np.float32)\n",
    "    norm = np.linalg.norm(vec)\n",
    "    if norm > 0: vec /= norm\n",
    "    return vec\n",
    "\n",
    "def process_video(video_info):\n",
    "    \"\"\"\n",
    "    Processes a single video, extracts features, and saves them.\n",
    "    This function is designed to be run by a multiprocessing worker.\n",
    "    \"\"\"\n",
    "    video_path = video_info['path']\n",
    "    output_dir = video_info['output_dir']\n",
    "    video_id = video_info['id'] # Added for logging\n",
    "    \n",
    "    # Ensure the detector is initialized in this worker process\n",
    "    global global_face_detector\n",
    "    if global_face_detector is None:\n",
    "        init_worker() # Fallback, should be handled by Pool(initializer=...)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True) # Ensure output dir exists for this video\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened(): \n",
    "        print(f\"[{current_process().name}] Warning: Could not open video {video_path}\")\n",
    "        return video_id, None, None # Return video_id and None for features and visualization\n",
    "        \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames < 1:\n",
    "        cap.release()\n",
    "        print(f\"[{current_process().name}] Warning: Video {video_path} has no frames.\")\n",
    "        return video_id, None, None\n",
    "\n",
    "    indices = np.linspace(0, total_frames - 1, MAX_FRAMES_PER_VIDEO, dtype=int) if total_frames > MAX_FRAMES_PER_VIDEO else np.arange(total_frames)\n",
    "    \n",
    "    # We only need to return data for the first real video for visualization\n",
    "    # Other videos just need to save their files.\n",
    "    temp_features = {'frames': [], 'illum_maps': [], 'material_maps': [], 'light_vectors': []}\n",
    "    is_first_real_video_of_session = False\n",
    "    \n",
    "    # If this is a real video and we need to save its features for potential visualization\n",
    "    if video_info['category'] == 'real' and not os.path.exists(os.path.join(output_dir, \"0000_frame.png\")):\n",
    "        is_first_real_video_of_session = True\n",
    "        \n",
    "    saved_frame_count = 0\n",
    "    \n",
    "    for frame_idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: continue\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Use the detector initialized by init_worker()\n",
    "        faces = global_face_detector.detect_faces(frame_rgb)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            best_face = sorted(faces, key=lambda f: f['confidence'], reverse=True)[0]\n",
    "            x, y, w, h = best_face['box']\n",
    "            x, y = max(0, x), max(0, y)\n",
    "            face_crop = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            if face_crop.size == 0: continue\n",
    "\n",
    "            face_crop_resized = cv2.resize(face_crop, IMG_SIZE)\n",
    "            \n",
    "            illum_map = extract_illumination_map_paper(face_crop_resized)\n",
    "            material_map = extract_face_material_map_paper(face_crop_resized)\n",
    "            light_vector = estimate_light_direction(face_crop_resized)\n",
    "            \n",
    "            fid = f\"{saved_frame_count:04d}\"\n",
    "            cv2.imwrite(os.path.join(output_dir, f\"{fid}_frame.png\"), face_crop_resized)\n",
    "            if illum_map is not None: cv2.imwrite(os.path.join(output_dir, f\"{fid}_illum.png\"), illum_map)\n",
    "            if material_map is not None: cv2.imwrite(os.path.join(output_dir, f\"{fid}_material.png\"), material_map)\n",
    "            np.save(os.path.join(output_dir, f\"{fid}_lightvec.npy\"), light_vector)\n",
    "            \n",
    "            # Only store features for the first frame of the first real video if needed for visualization\n",
    "            if is_first_real_video_of_session and saved_frame_count == 0:\n",
    "                 temp_features['frames'].append(face_crop_resized)\n",
    "                 temp_features['illum_maps'].append(illum_map)\n",
    "                 temp_features['material_maps'].append(material_map)\n",
    "                 temp_features['light_vectors'].append(light_vector)\n",
    "\n",
    "            saved_frame_count += 1\n",
    "    cap.release()\n",
    "    \n",
    "    if is_first_real_video_of_session and temp_features['frames']:\n",
    "        return video_id, temp_features, video_info['category']\n",
    "    return video_id, None, video_info['category']\n",
    "\n",
    "def visualize_preprocessing_steps(features):\n",
    "    if not features or not features['frames']: return\n",
    "    frame, illum_map, material_map, light_vector = features['frames'][0], features['illum_maps'][0], features['material_maps'][0], features['light_vectors'][0]\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle('Preprocessing Visualization (Corrected Implementation)', fontsize=16)\n",
    "    axes[0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)); axes[0].set_title('1. Original Face Crop'); axes[0].axis('off')\n",
    "    axes[1].imshow(illum_map); axes[1].set_title('2. Illumination Map'); axes[1].axis('off')\n",
    "    cx, cy = IMG_SIZE[0] // 2, IMG_SIZE[1] // 2\n",
    "    axes[1].arrow(cx, cy, light_vector[0] * 50, light_vector[1] * 50, head_width=10, head_length=10, fc='r', ec='r')\n",
    "    axes[2].imshow(material_map, cmap='gray'); axes[2].set_title('3. Face Material Map'); axes[2].axis('off')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def run_preprocessing():\n",
    "    \"\"\"Main function to run the entire preprocessing pipeline.\"\"\"\n",
    "    print(\"Starting preprocessing...\")\n",
    "\n",
    "    os.makedirs(PREPROCESSED_FEATURES_PATH, exist_ok=True)\n",
    "    progress_file = os.path.join(PREPROCESSED_FEATURES_PATH, '_progress.txt')\n",
    "    completed_videos = set()\n",
    "    try:\n",
    "        with open(progress_file, 'r') as f:\n",
    "            completed_videos = {line.strip() for line in f}\n",
    "        print(f\"Found {len(completed_videos)} videos already processed. Resuming.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Starting a new preprocessing run.\")\n",
    "        \n",
    "    # --- NO GLOBAL MTCNN INITIALIZATION HERE ---\n",
    "    # MTCNN will be initialized per worker process in init_worker()\n",
    "    \n",
    "    videos_to_process = []\n",
    "    for category, path in {'real': REAL_VIDEOS_PATH, 'fake': FAKE_VIDEOS_PATH}.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: Directory not found, skipping: {path}\")\n",
    "            continue\n",
    "        \n",
    "        video_files = [f for f in os.listdir(path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        if MAX_VIDEOS_PER_FOLDER is not None: video_files = video_files[:MAX_VIDEOS_PER_FOLDER]\n",
    "\n",
    "        for video_name in video_files:\n",
    "            video_id = os.path.splitext(video_name)[0]\n",
    "            video_unique_id = f\"{category}/{video_id}\"\n",
    "            \n",
    "            # Check if output directory already exists and contains files\n",
    "            video_output_dir = os.path.join(PREPROCESSED_FEATURES_PATH, category, video_id)\n",
    "            is_processed = os.path.exists(video_output_dir) and len(os.listdir(video_output_dir)) > 0\n",
    "\n",
    "            if not is_processed and video_unique_id not in completed_videos:\n",
    "                videos_to_process.append({\n",
    "                    'id': video_unique_id,\n",
    "                    'path': os.path.join(path, video_name),\n",
    "                    'output_dir': video_output_dir,\n",
    "                    'category': category\n",
    "                })\n",
    "            elif is_processed and video_unique_id not in completed_videos:\n",
    "                 # If files exist but not in progress log, add to log for consistency\n",
    "                 with open(progress_file, 'a') as progress_log:\n",
    "                     progress_log.write(f\"{video_unique_id}\\n\")\n",
    "                     progress_log.flush()\n",
    "                 completed_videos.add(video_unique_id)\n",
    "\n",
    "\n",
    "    if not videos_to_process:\n",
    "        print(\"All specified videos have already been processed. Nothing to do.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Total new videos to process: {len(videos_to_process)}\")\n",
    "\n",
    "    # --- Parallel Processing Setup ---\n",
    "    NUM_WORKERS = 32 # Set the number of workers\n",
    "    print(f\"Using {NUM_WORKERS} worker processes...\")\n",
    "    \n",
    "    first_real_video_features = None\n",
    "\n",
    "    # Use a multiprocessing Pool\n",
    "    # The initializer ensures MTCNN is created once per worker process\n",
    "    # The context 'spawn' is more robust across different OS (especially for macOS/Windows)\n",
    "    with Pool(processes=NUM_WORKERS, initializer=init_worker) as pool:\n",
    "        # Use imap_unordered for results as they become available and for the progress bar\n",
    "        for video_id, features_for_viz, category in tqdm(\n",
    "            pool.imap_unordered(process_video, videos_to_process),\n",
    "            total=len(videos_to_process),\n",
    "            desc=\"Processing Videos\"\n",
    "        ):\n",
    "            if video_id is not None:\n",
    "                with open(progress_file, 'a') as progress_log:\n",
    "                    progress_log.write(f\"{video_id}\\n\")\n",
    "                    progress_log.flush()\n",
    "                \n",
    "                if category == 'real' and features_for_viz is not None and first_real_video_features is None:\n",
    "                    first_real_video_features = features_for_viz\n",
    "\n",
    "    print(\"\\nPreprocessing complete.\")\n",
    "    if first_real_video_features:\n",
    "        print(\"Displaying visualization for the first real video processed in this session...\")\n",
    "        visualize_preprocessing_steps(first_real_video_features)\n",
    "    else:\n",
    "        print(\"No new real videos were processed in this session that required visualization data.\")\n",
    "\n",
    "# Ensure the script runs with multiprocessing\n",
    "if __name__ == '__main__':\n",
    "    run_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1b2b4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e7f15",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import timm\n",
    "import glob\n",
    "import json\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "class Config:\n",
    "    PREPROCESSED_DATA_DIR = \"/mnt/LIDeepDet Features\"\n",
    "    OUTPUT_DIR = \"Outputs\"\n",
    "    WANDB_PROJECT_NAME = \"LIDeepDet\"\n",
    "    WANDB_RUN_NAME = \"run-890-videos-weight-decay\"\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 1e-5\n",
    "    EPOCHS = 50\n",
    "    IMG_SIZE = (224, 224)\n",
    "    VIT_MODEL_NAME = 'vit_base_patch16_224'\n",
    "    EMBED_DIM = 768\n",
    "    RESUME_CHECKPOINT = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efa3f1",
   "metadata": {},
   "source": [
    "#### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDataset(Dataset):\n",
    "    # ... (This class is unchanged from the last robust version)\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        real_videos = [d for d in glob.glob(os.path.join(root_dir, 'real', '*')) if os.path.isdir(d)]\n",
    "        fake_videos = [d for d in glob.glob(os.path.join(root_dir, 'fake', '*')) if os.path.isdir(d)]\n",
    "        all_folders = [(d, 0) for d in real_videos] + [(d, 1) for d in fake_videos]\n",
    "        self.video_folders = [(path, label) for path, label in all_folders if len(glob.glob(os.path.join(path, '*_frame.png'))) > 0]\n",
    "        self.num_real = len(real_videos)\n",
    "        self.num_fake = len(fake_videos)\n",
    "        if len(all_folders) != len(self.video_folders): print(f\"Warning: Filtered out {len(all_folders) - len(self.video_folders)} empty video directories.\")\n",
    "    def __len__(self): return len(self.video_folders)\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir, label = self.video_folders[idx]\n",
    "        frame_files = glob.glob(os.path.join(video_dir, '*_frame.png'))\n",
    "        if not frame_files: return self.__getitem__((idx + 1) % len(self))\n",
    "        random_frame_path = np.random.choice(frame_files)\n",
    "        frame_id = os.path.basename(random_frame_path).split('_')[0]\n",
    "        rgb_path = random_frame_path\n",
    "        illum_path = os.path.join(video_dir, f\"{frame_id}_illum.png\")\n",
    "        material_path = os.path.join(video_dir, f\"{frame_id}_material.png\")\n",
    "        if not all(os.path.exists(p) for p in [rgb_path, illum_path, material_path]): return self.__getitem__((idx + 1) % len(self))\n",
    "        rgb_img = cv2.cvtColor(cv2.imread(rgb_path), cv2.COLOR_BGR2RGB)\n",
    "        illum_img = cv2.cvtColor(cv2.imread(illum_path), cv2.COLOR_BGR2RGB)\n",
    "        material_img = cv2.cvtColor(cv2.imread(material_path), cv2.COLOR_BGR2RGB)\n",
    "        if rgb_img is None or illum_img is None or material_img is None: return self.__getitem__((idx + 1) % len(self))\n",
    "        if self.transform:\n",
    "            rgb_img, illum_img, material_img = self.transform(rgb_img), self.transform(illum_img), self.transform(material_img)\n",
    "        return (rgb_img, illum_img, material_img), torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51c7a5",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.multihead_attn(query=query, key=key, value=value)\n",
    "        return attn_output\n",
    "\n",
    "class LIDeepDet(nn.Module):\n",
    "    def __init__(self, vit_model_name, embed_dim, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone_rgb = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.backbone_illum = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.backbone_material = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.cross_attention = CrossAttention(embed_dim)\n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(embed_dim * 6), nn.Linear(embed_dim * 6, embed_dim), nn.GELU(), nn.Linear(embed_dim, 1))\n",
    "\n",
    "    def forward(self, rgb_img, illum_img, material_img):\n",
    "        f_rgb = self.backbone_rgb.forward_features(rgb_img)[:, 0].unsqueeze(1)\n",
    "        f_illum = self.backbone_illum.forward_features(illum_img)[:, 0].unsqueeze(1)\n",
    "        f_material = self.backbone_material.forward_features(material_img)[:, 0].unsqueeze(1)\n",
    "        a_ri = self.cross_attention(f_rgb, f_illum, f_illum)\n",
    "        a_rm = self.cross_attention(f_rgb, f_material, f_material)\n",
    "        a_ir = self.cross_attention(f_illum, f_rgb, f_rgb)\n",
    "        a_im = self.cross_attention(f_illum, f_material, f_material)\n",
    "        a_mr = self.cross_attention(f_material, f_rgb, f_rgb)\n",
    "        a_mi = self.cross_attention(f_material, f_illum, f_illum)\n",
    "        fused_features = torch.cat([a_ri, a_rm, a_ir, a_im, a_mr, a_mi], dim=-1).squeeze(1)\n",
    "        return self.classifier(fused_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7405f",
   "metadata": {},
   "source": [
    "#### Training & Evaluation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb569c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and splitting dataset with extensive augmentations...\n",
      "Data split -> Train: 1424, Validation: 178, Test: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 19.56 GiB of which 25.19 MiB is free. Process 7259 has 6.07 GiB memory in use. Including non-PyTorch memory, this process has 12.85 GiB memory in use. Of the allocated memory 12.00 GiB is allocated by PyTorch, and 637.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 259\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    258\u001b[39m     os.makedirs(Config.OUTPUT_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 210\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# best_val_auc = 0.0 # This line is no longer strictly needed if using loss for early stopping\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config.EPOCHS):\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     train_metrics = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m     val_metrics = evaluate(model, val_loader, loss_fn, device)\n\u001b[32m    213\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_metrics[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_metrics[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, loss_fn, device)\u001b[39m\n\u001b[32m    105\u001b[39m labels = labels.to(device).unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m    106\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m loss = loss_fn(outputs, labels)\n\u001b[32m    109\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mLIDeepDet.forward\u001b[39m\u001b[34m(self, rgb, illum, material)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, rgb, illum, material):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     f_rgb, f_illum, f_mat = [\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m0\u001b[39m].unsqueeze(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m((\u001b[38;5;28mself\u001b[39m.backbone_rgb, \u001b[38;5;28mself\u001b[39m.backbone_illum, \u001b[38;5;28mself\u001b[39m.backbone_material), (rgb, illum, material))]\n\u001b[32m     73\u001b[39m     a_ri, _ = \u001b[38;5;28mself\u001b[39m.cross_attention(f_rgb, f_illum, f_illum)\n\u001b[32m     74\u001b[39m     a_rm, _ = \u001b[38;5;28mself\u001b[39m.cross_attention(f_rgb, f_mat, f_mat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/timm/models/vision_transformer.py:948\u001b[39m, in \u001b[36mVisionTransformer.forward_features\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    946\u001b[39m     x = checkpoint_seq(\u001b[38;5;28mself\u001b[39m.blocks, x)\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    950\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm(x)\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/timm/models/vision_transformer.py:177\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.Tensor:\n\u001b[32m    176\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path1(\u001b[38;5;28mself\u001b[39m.ls1(\u001b[38;5;28mself\u001b[39m.attn(\u001b[38;5;28mself\u001b[39m.norm1(x), attn_mask=attn_mask)))\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path2(\u001b[38;5;28mself\u001b[39m.ls2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/timm/layers/mlp.py:44\u001b[39m, in \u001b[36mMlp.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.act(x)\n\u001b[32m     46\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.drop1(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deepfake/LIDeepDet/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 19.56 GiB of which 25.19 MiB is free. Process 7259 has 6.07 GiB memory in use. Including non-PyTorch memory, this process has 12.85 GiB memory in use. Of the allocated memory 12.00 GiB is allocated by PyTorch, and 637.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import timm\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "class Config:\n",
    "    EPOCHS = 50\n",
    "    PATIENCE = 10\n",
    "    EMBED_DIM = 768\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 1e-5\n",
    "    IMG_SIZE = (224, 224)\n",
    "    OUTPUT_DIR = \"Outputs\"\n",
    "    RESUME_CHECKPOINT = None\n",
    "    # WANDB_PROJECT_NAME = \"LIDeepDet\"\n",
    "    PREPROCESSED_DATA_DIR = \"/mnt/LIDeepDet Features\"\n",
    "    VIT_MODEL_NAME = 'vit_base_patch16_224'\n",
    "    # WANDB_RUN_NAME = \"run-100-videos-weight-decay\"\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DATASET & MODEL (Unchanged)\n",
    "# ==============================================================================\n",
    "# The DeepfakeDataset and LIDeepDet classes are correct and do not need changes.\n",
    "# They are included here to make the script standalone.\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        real_videos = [d for d in glob.glob(os.path.join(root_dir, 'real', '*')) if os.path.isdir(d)]\n",
    "        fake_videos = [d for d in glob.glob(os.path.join(root_dir, 'fake', '*')) if os.path.isdir(d)]\n",
    "        all_folders = [(d, 0) for d in real_videos] + [(d, 1) for d in fake_videos]\n",
    "        self.video_folders = [(p, l) for p, l in all_folders if len(glob.glob(os.path.join(p, '*_frame.png'))) > 0]\n",
    "        self.num_real, self.num_fake = len(real_videos), len(fake_videos)\n",
    "        if len(all_folders) != len(self.video_folders): print(f\"Warning: Filtered {len(all_folders) - len(self.video_folders)} empty video directories.\")\n",
    "    def __len__(self): return len(self.video_folders)\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir, label = self.video_folders[idx]\n",
    "        frame_files = glob.glob(os.path.join(video_dir, '*_frame.png'))\n",
    "        if not frame_files: return self.__getitem__((idx + 1) % len(self))\n",
    "        frame_path = np.random.choice(frame_files)\n",
    "        frame_id = os.path.basename(frame_path).split('_')[0]\n",
    "        paths = [frame_path, os.path.join(video_dir, f\"{frame_id}_illum.png\"), os.path.join(video_dir, f\"{frame_id}_material.png\")]\n",
    "        if not all(os.path.exists(p) for p in paths): return self.__getitem__((idx + 1) % len(self))\n",
    "        images = [cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB) for p in paths]\n",
    "        if any(img is None for img in images): return self.__getitem__((idx + 1) % len(self))\n",
    "        if self.transform: images = [self.transform(img) for img in images]\n",
    "        return tuple(images), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "class LIDeepDet(nn.Module):\n",
    "    def __init__(self, vit_model_name, embed_dim, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone_rgb = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.backbone_illum = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.backbone_material = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim, 8, batch_first=True)\n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(embed_dim * 6), nn.Linear(embed_dim * 6, embed_dim), nn.GELU(), nn.Linear(embed_dim, 1))\n",
    "    def forward(self, rgb, illum, material):\n",
    "        f_rgb, f_illum, f_mat = [b.forward_features(x)[:, 0].unsqueeze(1) for b, x in zip((self.backbone_rgb, self.backbone_illum, self.backbone_material), (rgb, illum, material))]\n",
    "        a_ri, _ = self.cross_attention(f_rgb, f_illum, f_illum)\n",
    "        a_rm, _ = self.cross_attention(f_rgb, f_mat, f_mat)\n",
    "        a_ir, _ = self.cross_attention(f_illum, f_rgb, f_rgb)\n",
    "        a_im, _ = self.cross_attention(f_illum, f_mat, f_mat)\n",
    "        a_mr, _ = self.cross_attention(f_mat, f_rgb, f_rgb)\n",
    "        a_mi, _ = self.cross_attention(f_mat, f_illum, f_illum)\n",
    "        fused = torch.cat([a_ri, a_rm, a_ir, a_im, a_mr, a_mi], dim=-1).squeeze(1)\n",
    "        return self.classifier(fused)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. HELPER FUNCTIONS (ENHANCED METRICS & PLOTTING)\n",
    "# ==============================================================================\n",
    "def get_metrics(labels, preds):\n",
    "    \"\"\"Calculates a dictionary of metrics.\"\"\"\n",
    "    binary_preds = (preds > 0.5).astype(int)\n",
    "    metrics = {\n",
    "        \"accuracy\": np.mean(binary_preds == labels),\n",
    "        \"precision\": precision_score(labels, binary_preds, zero_division=0),\n",
    "        \"recall\": recall_score(labels, binary_preds, zero_division=0),\n",
    "        \"f1_score\": f1_score(labels, binary_preds, zero_division=0)\n",
    "    }\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        metrics[\"auc\"] = roc_auc_score(labels, preds)\n",
    "    else:\n",
    "        metrics[\"auc\"] = 0.5 # Default AUC if only one class is present\n",
    "    return metrics\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss, all_preds, all_labels = 0.0, [], []\n",
    "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        images = [img.to(device) for img in images]\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    metrics = get_metrics(np.array(all_labels), np.array(all_preds))\n",
    "    metrics['loss'] = total_loss / len(loader)\n",
    "    return metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_labels = 0.0, [], []\n",
    "    for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        images = [img.to(device) for img in images]\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        outputs = model(*images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    metrics = get_metrics(np.array(all_labels), np.array(all_preds))\n",
    "    metrics['loss'] = total_loss / len(loader)\n",
    "    metrics['predictions'] = np.array(all_preds)\n",
    "    metrics['labels'] = np.array(all_labels)\n",
    "    return metrics\n",
    "\n",
    "def log_diagnostic_plots(val_metrics, epoch):\n",
    "    \"\"\"Generates and logs confusion matrix and prediction histogram to W&B.\"\"\"\n",
    "    labels, preds = val_metrics['labels'].flatten(), val_metrics['predictions'].flatten()\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    cm = confusion_matrix(labels, preds > 0.5)\n",
    "    plt.figure(figsize=(8, 6)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "    plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title(f'Validation Confusion Matrix - Epoch {epoch}')\n",
    "    wandb.log({\"val_confusion_matrix\": wandb.Image(plt)}); plt.close()\n",
    "    \n",
    "    # 2. Prediction Distribution Histogram\n",
    "    plt.figure(figsize=(10, 6)); \n",
    "    sns.histplot(x=preds[labels==0], color='blue', alpha=0.5, label='Real Predictions', bins=30)\n",
    "    sns.histplot(x=preds[labels==1], color='red', alpha=0.5, label='Fake Predictions', bins=30)\n",
    "    plt.title(f'Validation Prediction Distribution - Epoch {epoch}'); plt.xlabel('Predicted Probability (of being Fake)'); plt.legend()\n",
    "    wandb.log({\"val_prediction_distribution\": wandb.Image(plt)}); plt.close()\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. MAIN FUNCTION\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    config = Config()\n",
    "    device = torch.device(config.DEVICE)\n",
    "    # wandb.init(project=config.WANDB_PROJECT_NAME, name=config.WANDB_RUN_NAME, config=vars(config))\n",
    "\n",
    "    # --- EXTENSIVE AUGMENTATION PIPELINE ---\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(), # Convert cv2 image to PIL Image for augmentations\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),\n",
    "        transforms.RandomResizedCrop(size=config.IMG_SIZE, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Validation and test sets should NOT have augmentations\n",
    "    eval_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(config.IMG_SIZE), # Just resize, no cropping\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"Loading and splitting dataset with extensive augmentations...\")\n",
    "    dataset_for_train = DeepfakeDataset(root_dir=config.PREPROCESSED_DATA_DIR, transform=train_transform)\n",
    "    dataset_for_eval = DeepfakeDataset(root_dir=config.PREPROCESSED_DATA_DIR, transform=eval_transform)\n",
    "    \n",
    "    train_size, val_size = int(0.8 * len(dataset_for_train)), int(0.1 * len(dataset_for_train))\n",
    "    test_size = len(dataset_for_train) - train_size - val_size\n",
    "    indices = torch.randperm(len(dataset_for_train)).tolist()\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(dataset_for_train, indices[:train_size])\n",
    "    val_dataset = torch.utils.data.Subset(dataset_for_eval, indices[train_size:train_size + val_size])\n",
    "    test_dataset = torch.utils.data.Subset(dataset_for_eval, indices[train_size + val_size:])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    print(f\"Data split -> Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "    model = LIDeepDet(config.VIT_MODEL_NAME, config.EMBED_DIM).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n",
    "    pos_weight = torch.tensor(dataset_for_train.num_real / dataset_for_train.num_fake if dataset_for_train.num_fake > 0 else 1, dtype=torch.float32).to(device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0 # <--- Counter for patience\n",
    "    \n",
    "    # best_val_auc = 0.0 # This line is no longer strictly needed if using loss for early stopping\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        train_metrics = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        val_metrics = evaluate(model, val_loader, loss_fn, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config.EPOCHS} -> Train Loss: {train_metrics['loss']:.4f}, Train Acc: {train_metrics['accuracy']:.4f} | Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        # wandb_logs = {\"epoch\": epoch + 1}\n",
    "        # for key, val in train_metrics.items(): wandb_logs[f\"train_{key}\"] = val\n",
    "        # for key, val in val_metrics.items(): \n",
    "            # if key not in ['predictions', 'labels']: wandb_logs[f\"val_{key}\"] = val\n",
    "        # wandb.log(wandb_logs)\n",
    "        \n",
    "        log_diagnostic_plots(val_metrics, epoch + 1)\n",
    "        \n",
    "        # --- Early Stopping Logic ---\n",
    "        if val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            epochs_no_improve = 0 # Reset counter\n",
    "            print(f\"  -> New best model found with Val Loss: {best_val_loss:.4f}. Saving lean checkpoint...\")\n",
    "            \n",
    "            best_model_path = os.path.join(config.OUTPUT_DIR, 'best_checkpoint.pth')\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            \n",
    "            # Log artifact only when a new best model is saved\n",
    "            # artifact = wandb.Artifact(f'best-model-run-{wandb.run.id}', type='model')\n",
    "            # artifact.add_file(best_model_path)\n",
    "            # wandb.log_artifact(artifact)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  -> Validation loss did not improve. Patience: {epochs_no_improve}/{config.PATIENCE}\")\n",
    "            if epochs_no_improve == config.PATIENCE:\n",
    "                print(f\"Early stopping triggered after {config.PATIENCE} epochs without improvement.\")\n",
    "                break # Exit the training loop\n",
    "\n",
    "    # --- Final Test Evaluation Logic (Unchanged) ---\n",
    "    print(\"\\n--- Training Finished. Starting Final Evaluation on Test Set ---\")\n",
    "    best_model_path = os.path.join(config.OUTPUT_DIR, 'best_checkpoint.pth')\n",
    "    if os.path.exists(best_model_path):\n",
    "        final_model = LIDeepDet(config.VIT_MODEL_NAME, config.EMBED_DIM).to(device)\n",
    "        final_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        test_metrics = evaluate(final_model, test_loader, loss_fn, device)\n",
    "        print(f\"Final Test Set Results -> Accuracy: {test_metrics['accuracy']:.4f}, AUC: {test_metrics['auc']:.4f}, F1: {test_metrics['f1_score']:.4f}\")\n",
    "        # wandb.log({f\"test_{k}\": v for k, v in test_metrics.items() if k not in ['predictions', 'labels']})\n",
    "    else:\n",
    "        print(\"No best model checkpoint found (training might have stopped too early without any improvement).\")\n",
    "    \n",
    "    # wandb.finish()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
